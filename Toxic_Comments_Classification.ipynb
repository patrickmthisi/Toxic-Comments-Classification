{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a99a2db",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n",
    "\n",
    "- toxic\n",
    "- severe_toxic\n",
    "- obscene\n",
    "- threat\n",
    "- insult\n",
    "- identity_hate\n",
    "\n",
    "You must create a model which predicts a probability of each type of toxicity for each comment.\n",
    "\n",
    "File descriptions:\n",
    "- train.csv - the training set, contains comments with their binary labels\n",
    "- test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set - contains some comments which are not included in scoring.\n",
    "- sample_submission.csv - a sample submission file in the correct format\n",
    "- test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272db35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "from pickle import dump, load\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Conv1D, MaxPool1D, Flatten, Dropout, Dense, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def extract_zip_files(zip_file_path, extract_to_dir):\n",
    "    # Path to the zip file\n",
    "    zip_file_path = zip_file_path\n",
    "    # directory to extract contents\n",
    "    extract_to_dir = extract_to_dir\n",
    "    # Extracting the zip file\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            # Extract all contents to the specified directory\n",
    "            zip_ref.extractall(extract_to_dir)\n",
    "        print(f\"Contents of {zip_file_path} extracted successfully to {extract_to_dir}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {zip_file_path} does not exist.\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: The file {zip_file_path} is not a valid zip file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from words\n",
    "    tokens = [re_punc.sub('', word) for word in tokens]\n",
    "    # remove remaining words that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # standardize tokens\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# save a dataset to a file\n",
    "def save_dataset(dataset, filename):\n",
    "    dump(dataset, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "    \n",
    "# load a clean dataset\n",
    "def load_output(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2  # Memory usage in MB\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            # Downcast integers\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            # Downcast floats\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        reduction_percent = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Memory usage decreased to {end_mem:.2f} MB ({reduction_percent:.1f}% reduction)\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7584e4b",
   "metadata": {},
   "source": [
    "# Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b1b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract main file\n",
    "zip_file_path = 'jigsaw-toxic-comment-classification-challenge.zip'\n",
    "extract_to_dir = './toxic_files'\n",
    "extract_zip_files(zip_file_path, extract_to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb31f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract training zip file:\n",
    "zip_file_path = './toxic_files/train.csv.zip'\n",
    "extract_to_dir = './toxic_files/train_file'\n",
    "extract_zip_files(zip_file_path, extract_to_dir)\n",
    "\n",
    "# extract test zip file:\n",
    "zip_file_path = './toxic_files/test.csv.zip'\n",
    "extract_to_dir = './toxic_files/test_file'\n",
    "extract_zip_files(zip_file_path, extract_to_dir)\n",
    "\n",
    "# extract \n",
    "zip_file_path = './toxic_files/test_labels.csv.zip'\n",
    "extract_to_dir = './toxic_files/test_file'\n",
    "extract_zip_files(zip_file_path, extract_to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339fbf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load extracted files\n",
    "train_df = pd.read_csv('./toxic_files/train_file/train.csv', index_col=0)\n",
    "test_doc = pd.read_csv('./toxic_files/test_file/test.csv')\n",
    "ytest = pd.read_csv('./toxic_files/test_file/test_labels.csv')\n",
    "\n",
    "train_df = reduce_mem_usage(train_df)\n",
    "test_doc = reduce_mem_usage(test_doc)\n",
    "ytest = reduce_mem_usage(ytest)\n",
    "\n",
    "train_df.info()\n",
    "\n",
    "# perform data remediation for the test set:\n",
    "test_df = pd.merge(test_doc, ytest, on='id', how='left')\n",
    "test_df.set_index('id', inplace=True)\n",
    "# remove none informative labels\n",
    "test_df = test_df[~(test_df == -1).any(axis=1)]\n",
    "# sumarise data sets:\n",
    "print('\\nData summary:')\n",
    "print(f'Training dataset: {train_df.shape[0]} rows, {train_df.shape[1]} columns')\n",
    "print(f'Test dataset: {test_df.shape[0]} rows, {test_df.shape[1]} columns')\n",
    "\n",
    "# split into input and output:\n",
    "train_docs, ytrain = train_df['comment_text'], train_df.iloc[:, -6:].values.astype(int)\n",
    "test_docs, ytest = test_df['comment_text'], test_df.iloc[:, -6:].values.astype(int)\n",
    "\n",
    "# save files:\n",
    "train_docs = train_docs.to_frame()\n",
    "test_docs = test_docs.to_frame()\n",
    "train_docs.to_csv('train_docs.csv'), test_docs.to_csv('test_docs.csv')\n",
    "save_dataset([ytrain, ytest], 'output.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b750ed5",
   "metadata": {},
   "source": [
    "# Analysing most toxic tokens by toxicity type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class toxicity_analyser(object):\n",
    "    def __init__(self, df, most_common):\n",
    "        self.df = df\n",
    "        self.most_common = most_common\n",
    "        \n",
    "    def combine_words(self, word_list):\n",
    "        all_words = []\n",
    "        for word in word_list:\n",
    "            all_words += word\n",
    "        return all_words\n",
    "        \n",
    "    def create_word_cloud(self, col_name):\n",
    "        df = self.df[self.df[col_name] == 1]\n",
    "        tokens = df['comment_text'].apply(clean_doc)\n",
    "        reviewed_tokens = self.combine_words(tokens)\n",
    "        mostcommon = FreqDist(reviewed_tokens).most_common(self.most_common)\n",
    "        wordcloud = WordCloud(width=1500, height=800, background_color='white').generate(str(mostcommon))\n",
    "        fig = plt.figure(figsize=(30,10), facecolor='white')\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Top {self.most_common} Most Common Words', fontsize=25)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity = toxicity_analyser(train_df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d61d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94179a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('severe_toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('obscene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('threat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('insult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('identity_hate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f574952",
   "metadata": {},
   "source": [
    "# Create Vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add doc to vocabulary\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines into a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "\n",
    "# add all docs to vocab\n",
    "add_doc_to_vocab('train_docs.csv', vocab)\n",
    "\n",
    "# print size of the vocab\n",
    "print(len(vocab))\n",
    "# print most common words - overall\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "# keep token with a min occurance\n",
    "min_occurance = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "print(len(tokens))\n",
    "\n",
    "# save token to vocavulary\n",
    "save_list(tokens, 'vocab.text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed411c41",
   "metadata": {},
   "source": [
    "# Build model: n-gram multihead CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check if functions are defined\n",
    "print(\"Before defining functions:\")\n",
    "print(\"clean_doc:\", type(clean_doc) if 'clean_doc' in globals() else \"Not defined\")\n",
    "print(\"load_doc:\", type(load_doc) if 'load_doc' in globals() else \"Not defined\")\n",
    "print(\"load_output:\", type(load_output) if 'load_output' in globals() else \"Not defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b646feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length, batch_size=32):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.texts) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.texts[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        encoded_texts = encode_text(self.tokenizer, batch_texts, self.max_length)\n",
    "        return [encoded_texts] * len(kernels), np.array(batch_labels)\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()\n",
    "    \n",
    "# load a clean dataset\n",
    "def load_output(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "    \n",
    "    \n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from words\n",
    "    tokens = [re_punc.sub('', word) for word in tokens]\n",
    "    # remove remaining words that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # standardize tokens\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    #filter out tokens not in the vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(filename, vocab):\n",
    "    doc = load_doc(filename)  # Ensure `load_doc` is defined\n",
    "    tokens = clean_doc(doc, vocab)\n",
    "    return tokens\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded\n",
    "\n",
    "# Define model\n",
    "def define_model(vocab_size, length, kernels, weights, embedding_dim=100):\n",
    "    inputs = []\n",
    "    combine = []\n",
    "    for kernel in kernels:\n",
    "        in_layer = Input(shape=(length,))\n",
    "        embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[weights], trainable=True)(in_layer)\n",
    "        conv = Conv1D(filters=32, kernel_size=kernel, activation='relu')(embedding)\n",
    "        drop = Dropout(0.5)(conv)\n",
    "        pool = MaxPool1D(pool_size=2)(drop)\n",
    "        flat = Flatten()(pool)\n",
    "        inputs.append(in_layer)\n",
    "        combine.append(flat)\n",
    "    merged = concatenate(combine)\n",
    "    dense = Dense(100, activation='relu')(merged)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "    outputs = Dense(6, activation='softmax')(dense)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=1000, decay_rate=0.9)\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', AUC(name='roc_auc')])\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='n_gram_multihead_cnn_model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, Xtrain, ytrain, Xtest, ytest, kernels, tokenizer, length, n_repeats=5):\n",
    "    train_generator = DataGenerator(Xtrain, ytrain, tokenizer, length, batch_size=32)\n",
    "    test_generator = DataGenerator(Xtest, ytest, tokenizer, length, batch_size=32)\n",
    "    acc_scores = []\n",
    "    auc_scores = []\n",
    "    for i in range(1, n_repeats + 1):\n",
    "        rlp = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5)\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', patience=5, restore_best_weights=True)\n",
    "        model.fit(train_generator, validation_data=test_generator, epochs=20, verbose=1, callbacks=[rlp, es])\n",
    "        metrics = model.evaluate(test_generator, verbose=0)\n",
    "        acc = metrics[1] \n",
    "        auc = metrics[2]  \n",
    "        if i == 1 or i % 5 == 0:\n",
    "            print(f'Run {i}: Test Accuracy: {acc * 100:.3f}, Test ROC AUC: {auc * 100:.3f}')\n",
    "        acc_scores.append(acc)\n",
    "        auc_scores.append(auc)\n",
    "    mean_acc = np.mean(acc_scores)\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    print(f'Mean Accuracy: {mean_acc * 100:.3f}, Mean ROC AUC: {mean_auc * 100:.3f}')\n",
    "    \n",
    "# load vocab\n",
    "vocab_filename = 'vocab.text'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "    \n",
    "# Load vocab\n",
    "vocab_filename = 'vocab.text'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "# Load datasets\n",
    "ytrain, ytest = load_output('output.pkl')  # Ensure these are numpy arrays with correct shape\n",
    "\n",
    "# Clean training and test docs\n",
    "trainLines = train_docs['comment_text'].apply(lambda x: clean_doc(x, vocab)).astype(str)\n",
    "testLines = test_docs['comment_text'].apply(lambda x: clean_doc(x, vocab)).astype(str)\n",
    "\n",
    "# Tokenize and encode datasets\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "length = max_length(trainLines)\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "print(f'Maximum document length: {length}')\n",
    "\n",
    "\n",
    "# load the embedding into memory\n",
    "embeddings_index = {}\n",
    "f = open(\n",
    "    '/home/pmthisi/code_env/6_Deep_Learning_for_Natural_Language_Processing/glove_files/glove.6B.100d.txt', \n",
    "    mode='rt', \n",
    "    encoding='utf-8'\n",
    ")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vector' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix for words in the training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Define and evaluate model\n",
    "kernels = [2, 3]\n",
    "model = define_model(vocab_size, length, kernels, embedding_matrix)\n",
    "evaluate_model(model, trainLines.tolist(), ytrain, testLines.tolist(), ytest, kernels, tokenizer, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb3913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
