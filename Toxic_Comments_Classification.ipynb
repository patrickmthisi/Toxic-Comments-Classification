{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a99a2db",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n",
    "\n",
    "- toxic\n",
    "- severe_toxic\n",
    "- obscene\n",
    "- threat\n",
    "- insult\n",
    "- identity_hate\n",
    "\n",
    "You must create a model which predicts a probability of each type of toxicity for each comment.\n",
    "\n",
    "File descriptions:\n",
    "- train.csv - the training set, contains comments with their binary labels\n",
    "- test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set - contains some comments which are not included in scoring.\n",
    "- sample_submission.csv - a sample submission file in the correct format\n",
    "- test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272db35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "from pickle import dump, load\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPool1D, Bidirectional, LSTM, Flatten, Dense, concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import Sequence, plot_model\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def extract_zip_files(zip_file_path, extract_to_dir):\n",
    "    # Path to the zip file\n",
    "    zip_file_path = zip_file_path\n",
    "    # directory to extract contents\n",
    "    extract_to_dir = extract_to_dir\n",
    "    # Extracting the zip file\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            # Extract all contents to the specified directory\n",
    "            zip_ref.extractall(extract_to_dir)\n",
    "        print(f\"Contents of {zip_file_path} extracted successfully to {extract_to_dir}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {zip_file_path} does not exist.\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: The file {zip_file_path} is not a valid zip file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()\n",
    "    \n",
    "\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('', word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha() or word in ['not', 'no']]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# save a dataset to a file\n",
    "def save_dataset(dataset, filename):\n",
    "    dump(dataset, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "    \n",
    "# load a clean dataset\n",
    "def load_output(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2  # Memory usage in MB\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            # Downcast integers\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            # Downcast floats\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        reduction_percent = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Memory usage decreased to {end_mem:.2f} MB ({reduction_percent:.1f}% reduction)\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7584e4b",
   "metadata": {},
   "source": [
    "# Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0b1b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of jigsaw-toxic-comment-classification-challenge.zip extracted successfully to ./toxic_files.\n"
     ]
    }
   ],
   "source": [
    "# extract main file\n",
    "zip_file_path = 'jigsaw-toxic-comment-classification-challenge.zip'\n",
    "extract_to_dir = './toxic_files'\n",
    "extract_zip_files(zip_file_path, extract_to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb31f6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of ./toxic_files/train.csv.zip extracted successfully to ./toxic_files/train_file.\n",
      "Contents of ./toxic_files/test.csv.zip extracted successfully to ./toxic_files/test_file.\n",
      "Contents of ./toxic_files/test_labels.csv.zip extracted successfully to ./toxic_files/test_file.\n"
     ]
    }
   ],
   "source": [
    "# extract training zip file:\n",
    "zip_file_path = './toxic_files/train.csv.zip'\n",
    "extract_to_dir = './toxic_files/train_file'\n",
    "extract_zip_files(zip_file_path, extract_to_dir)\n",
    "\n",
    "# extract test zip file:\n",
    "zip_file_path = './toxic_files/test.csv.zip'\n",
    "extract_to_dir = './toxic_files/test_file'\n",
    "extract_zip_files(zip_file_path, extract_to_dir)\n",
    "\n",
    "# extract \n",
    "zip_file_path = './toxic_files/test_labels.csv.zip'\n",
    "extract_to_dir = './toxic_files/test_file'\n",
    "extract_zip_files(zip_file_path, extract_to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "339fbf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage decreased to 3.35 MB (65.6% reduction)\n",
      "Memory usage decreased to 2.34 MB (0.0% reduction)\n",
      "Memory usage decreased to 2.05 MB (75.0% reduction)\n",
      "\n",
      "Data summary:\n",
      "Training dataset: 159571 rows, 7 columns\n",
      "Test dataset: 63978 rows, 7 columns\n",
      "Saved: output.pkl\n",
      "Saved: input.pkl\n"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "train_df = pd.read_csv('./toxic_files/train_file/train.csv', index_col=0)\n",
    "test_doc = pd.read_csv('./toxic_files/test_file/test.csv')\n",
    "ytest = pd.read_csv('./toxic_files/test_file/test_labels.csv')\n",
    "\n",
    "# reduce memory:\n",
    "train_df = reduce_mem_usage(train_df)\n",
    "test_doc = reduce_mem_usage(test_doc)\n",
    "ytest = reduce_mem_usage(ytest)\n",
    "\n",
    "# perform data remediation for the test set:\n",
    "test_df = pd.merge(test_doc, ytest, on='id', how='left')\n",
    "test_df.set_index('id', inplace=True)\n",
    "\n",
    "# remove none informative labels\n",
    "test_df = test_df[~(test_df == -1).any(axis=1)]\n",
    "\n",
    "# sumarise data sets:\n",
    "print('\\nData summary:')\n",
    "print(f'Training dataset: {train_df.shape[0]} rows, {train_df.shape[1]} columns')\n",
    "print(f'Test dataset: {test_df.shape[0]} rows, {test_df.shape[1]} columns')\n",
    "\n",
    "# split into input and output:\n",
    "train_docs, ytrain = train_df['comment_text'], train_df.iloc[:, -6:].values.astype(int)\n",
    "test_docs, ytest = test_df['comment_text'], test_df.iloc[:, -6:].values.astype(int)\n",
    "\n",
    "# save files:\n",
    "train_docs = train_docs.to_frame()\n",
    "test_docs = test_docs.to_frame()\n",
    "train_docs.to_csv('train_docs.csv'), test_docs.to_csv('test_docs.csv')\n",
    "save_dataset([ytrain, ytest], 'output.pkl')\n",
    "save_dataset([train_docs, test_docs], 'input.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b750ed5",
   "metadata": {},
   "source": [
    "# Analysing toxicity by type: \n",
    "#### NOTE!!!! Cells in this section are not executed due to harmful words which flags this notebook as harmful on Github!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa6ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class toxicity_analyser(object):\n",
    "    def __init__(self, df, most_common):\n",
    "        self.df = df\n",
    "        self.most_common = most_common\n",
    "        \n",
    "    def combine_words(self, word_list):\n",
    "        all_words = []\n",
    "        for word in word_list:\n",
    "            all_words += word\n",
    "        return all_words\n",
    "        \n",
    "    def create_word_cloud(self, col_name):\n",
    "        df = self.df[self.df[col_name] == 1]\n",
    "        tokens = df['comment_text'].apply(clean_doc)\n",
    "        reviewed_tokens = self.combine_words(tokens)\n",
    "        mostcommon = FreqDist(reviewed_tokens).most_common(self.most_common)\n",
    "        wordcloud = WordCloud(width=1500, height=800, background_color='white').generate(str(mostcommon))\n",
    "        fig = plt.figure(figsize=(30,10), facecolor='white')\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Top {self.most_common} Most Common Words', fontsize=25)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da34275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity = toxicity_analyser(train_df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d61d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94179a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('severe_toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('obscene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('threat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('insult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('identity_hate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f574952",
   "metadata": {},
   "source": [
    "# Create Vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f11d9adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212766\n",
      "[('the', 491916), ('to', 296460), ('of', 223846), ('and', 221212), ('you', 201608), ('is', 175487), ('that', 153679), ('in', 143493), ('it', 128298), ('for', 102129), ('this', 95292), ('not', 92909), ('on', 89218), ('be', 83262), ('as', 76724), ('have', 72026), ('are', 71536), ('your', 62209), ('with', 59396), ('if', 57238), ('article', 55229), ('was', 54418), ('or', 52087), ('but', 50416), ('page', 45521), ('my', 44884), ('an', 44347), ('from', 41275), ('by', 40812), ('do', 39326), ('at', 39244), ('about', 36915), ('me', 36821), ('wikipedia', 35279), ('so', 35247), ('can', 33684), ('what', 32598), ('there', 31207), ('all', 30974), ('talk', 30801), ('has', 30653), ('will', 30327), ('would', 29103), ('its', 28186), ('one', 27880), ('please', 27658), ('like', 27641), ('no', 27355), ('just', 27261), ('they', 27000)]\n",
      "89418\n"
     ]
    }
   ],
   "source": [
    "# add doc to vocabulary\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close\n",
    "\n",
    "vocab = Counter()\n",
    "add_doc_to_vocab('train_docs.csv', vocab)\n",
    "print(len(vocab))\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "min_occurance = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "print(len(tokens))\n",
    "# save token to vocavulary\n",
    "save_list(tokens, 'vocab.text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed411c41",
   "metadata": {},
   "source": [
    "# Build model: n-gram Bidirectional CNN-LSTM Model with Multihead Attention & GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca64fd82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 89419\n",
      "Average document length: 62\n",
      "Loaded 400000 word vector\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 62)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 62)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 62)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 62, 100)              8941900   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 62, 100)              8941900   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, 62, 100)              8941900   ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 62, 100)              400       ['embedding[0][0]']           \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 62, 100)              400       ['embedding_1[0][0]']         \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 62, 100)              400       ['embedding_2[0][0]']         \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 60, 32)               9632      ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 58, 32)               16032     ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 56, 32)               22432     ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 60, 32)               0         ['conv1d[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 58, 32)               0         ['conv1d_1[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 56, 32)               0         ['conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1  (None, 30, 32)               0         ['dropout[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPoolin  (None, 29, 32)               0         ['dropout_1[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPoolin  (None, 28, 32)               0         ['dropout_2[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 960)                  0         ['max_pooling1d[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 928)                  0         ['max_pooling1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)         (None, 896)                  0         ['max_pooling1d_2[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 2784)                 0         ['flatten[0][0]',             \n",
      "                                                                     'flatten_1[0][0]',           \n",
      "                                                                     'flatten_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 50)                   139250    ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 50)                   200       ['dense[0][0]']               \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 50)                   0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 6)                    306       ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27014752 (103.05 MB)\n",
      "Trainable params: 27014052 (103.05 MB)\n",
      "Non-trainable params: 700 (2.73 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.8916 - accuracy: 0.2636 - roc_auc: 0.5272\n",
      "Epoch 1: val_roc_auc improved from -inf to 0.50891, saving model to best_model_iter_1.keras\n",
      "2494/2494 [==============================] - 473s 189ms/step - loss: 0.8916 - accuracy: 0.2636 - roc_auc: 0.5272 - val_loss: 0.7398 - val_accuracy: 0.9942 - val_roc_auc: 0.5089 - lr: 7.6900e-04\n",
      "Epoch 2/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 1.1033 - accuracy: 0.2676 - roc_auc: 0.5612\n",
      "Epoch 2: val_roc_auc improved from 0.50891 to 0.56784, saving model to best_model_iter_1.keras\n",
      "2494/2494 [==============================] - 472s 189ms/step - loss: 1.1033 - accuracy: 0.2676 - roc_auc: 0.5612 - val_loss: 0.7236 - val_accuracy: 0.9975 - val_roc_auc: 0.5678 - lr: 5.9130e-04\n",
      "Epoch 3/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 1.0200 - accuracy: 0.2683 - roc_auc: 0.5823\n",
      "Epoch 3: val_roc_auc improved from 0.56784 to 0.60097, saving model to best_model_iter_1.keras\n",
      "2494/2494 [==============================] - 469s 188ms/step - loss: 1.0200 - accuracy: 0.2683 - roc_auc: 0.5823 - val_loss: 0.6976 - val_accuracy: 0.9638 - val_roc_auc: 0.6010 - lr: 4.5466e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9537 - accuracy: 0.2688 - roc_auc: 0.5954\n",
      "Epoch 4: val_roc_auc improved from 0.60097 to 0.62397, saving model to best_model_iter_1.keras\n",
      "2494/2494 [==============================] - 473s 190ms/step - loss: 0.9537 - accuracy: 0.2688 - roc_auc: 0.5954 - val_loss: 0.6863 - val_accuracy: 0.9953 - val_roc_auc: 0.6240 - lr: 3.4960e-04\n",
      "Epoch 5/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9179 - accuracy: 0.2683 - roc_auc: 0.6057\n",
      "Epoch 5: val_roc_auc did not improve from 0.62397\n",
      "2494/2494 [==============================] - 468s 188ms/step - loss: 0.9179 - accuracy: 0.2683 - roc_auc: 0.6057 - val_loss: 0.7501 - val_accuracy: 0.9868 - val_roc_auc: 0.6159 - lr: 2.6881e-04\n",
      "Epoch 6/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.8962 - accuracy: 0.2687 - roc_auc: 0.6142\n",
      "Epoch 6: val_roc_auc improved from 0.62397 to 0.65997, saving model to best_model_iter_1.keras\n",
      "2494/2494 [==============================] - 468s 188ms/step - loss: 0.8962 - accuracy: 0.2687 - roc_auc: 0.6142 - val_loss: 0.7125 - val_accuracy: 0.9970 - val_roc_auc: 0.6600 - lr: 2.0670e-04\n",
      "Epoch 7/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.8935 - accuracy: 0.2674 - roc_auc: 0.6211\n",
      "Epoch 7: val_roc_auc did not improve from 0.65997\n",
      "2494/2494 [==============================] - 470s 188ms/step - loss: 0.8935 - accuracy: 0.2674 - roc_auc: 0.6211 - val_loss: 0.7491 - val_accuracy: 0.9932 - val_roc_auc: 0.6561 - lr: 1.5893e-04\n",
      "Epoch 8/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.8943 - accuracy: 0.2699 - roc_auc: 0.6266\n",
      "Epoch 8: val_roc_auc improved from 0.65997 to 0.66973, saving model to best_model_iter_1.keras\n",
      "2494/2494 [==============================] - 472s 189ms/step - loss: 0.8943 - accuracy: 0.2699 - roc_auc: 0.6266 - val_loss: 0.7393 - val_accuracy: 0.9961 - val_roc_auc: 0.6697 - lr: 1.2221e-04\n",
      "Epoch 9/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.8950 - accuracy: 0.2684 - roc_auc: 0.6287\n",
      "Epoch 9: val_roc_auc improved from 0.66973 to 0.67637, saving model to best_model_iter_1.keras\n",
      "2494/2494 [==============================] - 472s 189ms/step - loss: 0.8950 - accuracy: 0.2684 - roc_auc: 0.6287 - val_loss: 0.7361 - val_accuracy: 0.9971 - val_roc_auc: 0.6764 - lr: 9.3966e-05\n",
      "Epoch 10/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9005 - accuracy: 0.2701 - roc_auc: 0.6333\n",
      "Epoch 10: val_roc_auc did not improve from 0.67637\n",
      "2494/2494 [==============================] - 471s 189ms/step - loss: 0.9005 - accuracy: 0.2701 - roc_auc: 0.6333 - val_loss: 0.7585 - val_accuracy: 0.9971 - val_roc_auc: 0.6762 - lr: 7.2253e-05\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.7585 - accuracy: 0.9971 - roc_auc: 0.6762\n",
      "> run=1: Accuracy=99.706, ROC AUC=67.617\n",
      "Epoch 1/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9115 - accuracy: 0.2687 - roc_auc: 0.6339\n",
      "Epoch 1: val_roc_auc improved from -inf to 0.67596, saving model to best_model_iter_2.keras\n",
      "2494/2494 [==============================] - 476s 191ms/step - loss: 0.9115 - accuracy: 0.2687 - roc_auc: 0.6339 - val_loss: 0.7516 - val_accuracy: 0.9969 - val_roc_auc: 0.6760 - lr: 5.5556e-05\n",
      "Epoch 2/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9250 - accuracy: 0.2699 - roc_auc: 0.6336\n",
      "Epoch 2: val_roc_auc improved from 0.67596 to 0.67620, saving model to best_model_iter_2.keras\n",
      "2494/2494 [==============================] - 473s 190ms/step - loss: 0.9250 - accuracy: 0.2699 - roc_auc: 0.6336 - val_loss: 0.7708 - val_accuracy: 0.9969 - val_roc_auc: 0.6762 - lr: 4.2718e-05\n",
      "Epoch 3/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9218 - accuracy: 0.2686 - roc_auc: 0.6374\n",
      "Epoch 3: val_roc_auc did not improve from 0.67620\n",
      "2494/2494 [==============================] - 473s 190ms/step - loss: 0.9218 - accuracy: 0.2686 - roc_auc: 0.6374 - val_loss: 0.8056 - val_accuracy: 0.9965 - val_roc_auc: 0.6732 - lr: 3.2847e-05\n",
      "Epoch 4/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9289 - accuracy: 0.2671 - roc_auc: 0.6372\n",
      "Epoch 4: val_roc_auc did not improve from 0.67620\n",
      "2494/2494 [==============================] - 474s 190ms/step - loss: 0.9289 - accuracy: 0.2671 - roc_auc: 0.6372 - val_loss: 0.7903 - val_accuracy: 0.9966 - val_roc_auc: 0.6753 - lr: 2.5257e-05\n",
      "Epoch 5/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9406 - accuracy: 0.2667 - roc_auc: 0.6370\n",
      "Epoch 5: val_roc_auc improved from 0.67620 to 0.67622, saving model to best_model_iter_2.keras\n",
      "2494/2494 [==============================] - 483s 194ms/step - loss: 0.9406 - accuracy: 0.2667 - roc_auc: 0.6370 - val_loss: 0.7942 - val_accuracy: 0.9967 - val_roc_auc: 0.6762 - lr: 1.9420e-05\n",
      "Epoch 6/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9410 - accuracy: 0.2658 - roc_auc: 0.6390\n",
      "Epoch 6: val_roc_auc did not improve from 0.67622\n",
      "2494/2494 [==============================] - 480s 192ms/step - loss: 0.9410 - accuracy: 0.2658 - roc_auc: 0.6390 - val_loss: 0.8197 - val_accuracy: 0.9966 - val_roc_auc: 0.6754 - lr: 1.4933e-05\n",
      "Epoch 7/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9527 - accuracy: 0.2646 - roc_auc: 0.6358\n",
      "Epoch 7: val_roc_auc did not improve from 0.67622\n",
      "2494/2494 [==============================] - 471s 189ms/step - loss: 0.9527 - accuracy: 0.2646 - roc_auc: 0.6358 - val_loss: 0.8082 - val_accuracy: 0.9958 - val_roc_auc: 0.6722 - lr: 1.1482e-05\n",
      "Epoch 8/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9504 - accuracy: 0.2632 - roc_auc: 0.6363\n",
      "Epoch 8: val_roc_auc did not improve from 0.67622\n",
      "2494/2494 [==============================] - 472s 189ms/step - loss: 0.9504 - accuracy: 0.2632 - roc_auc: 0.6363 - val_loss: 0.8392 - val_accuracy: 0.9961 - val_roc_auc: 0.6722 - lr: 8.8288e-06\n",
      "1000/1000 [==============================] - 6s 5ms/step - loss: 0.7942 - accuracy: 0.9967 - roc_auc: 0.6762\n",
      "> run=2: Accuracy=99.666, ROC AUC=67.622\n",
      "Epoch 1/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9436 - accuracy: 0.2636 - roc_auc: 0.6359\n",
      "Epoch 1: val_roc_auc improved from -inf to 0.67317, saving model to best_model_iter_3.keras\n",
      "2494/2494 [==============================] - 473s 190ms/step - loss: 0.9436 - accuracy: 0.2636 - roc_auc: 0.6359 - val_loss: 0.8160 - val_accuracy: 0.9961 - val_roc_auc: 0.6732 - lr: 6.7886e-06\n",
      "Epoch 2/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9531 - accuracy: 0.2621 - roc_auc: 0.6336\n",
      "Epoch 2: val_roc_auc improved from 0.67317 to 0.67320, saving model to best_model_iter_3.keras\n",
      "2494/2494 [==============================] - 473s 190ms/step - loss: 0.9531 - accuracy: 0.2621 - roc_auc: 0.6336 - val_loss: 0.7926 - val_accuracy: 0.9958 - val_roc_auc: 0.6732 - lr: 5.2199e-06\n",
      "Epoch 3/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9507 - accuracy: 0.2613 - roc_auc: 0.6349\n",
      "Epoch 3: val_roc_auc did not improve from 0.67320\n",
      "2494/2494 [==============================] - 472s 189ms/step - loss: 0.9507 - accuracy: 0.2613 - roc_auc: 0.6349 - val_loss: 0.8055 - val_accuracy: 0.9959 - val_roc_auc: 0.6727 - lr: 4.0137e-06\n",
      "Epoch 4/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9522 - accuracy: 0.2602 - roc_auc: 0.6344\n",
      "Epoch 4: val_roc_auc did not improve from 0.67320\n",
      "2494/2494 [==============================] - 468s 188ms/step - loss: 0.9522 - accuracy: 0.2602 - roc_auc: 0.6344 - val_loss: 0.8367 - val_accuracy: 0.9960 - val_roc_auc: 0.6716 - lr: 3.0862e-06\n",
      "Epoch 5/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9531 - accuracy: 0.2591 - roc_auc: 0.6350\n",
      "Epoch 5: val_roc_auc did not improve from 0.67320\n",
      "2494/2494 [==============================] - 471s 189ms/step - loss: 0.9531 - accuracy: 0.2591 - roc_auc: 0.6350 - val_loss: 0.8387 - val_accuracy: 0.9960 - val_roc_auc: 0.6716 - lr: 2.3730e-06\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.7926 - accuracy: 0.9958 - roc_auc: 0.6732\n",
      "> run=3: Accuracy=99.583, ROC AUC=67.320\n",
      "Epoch 1/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9495 - accuracy: 0.2592 - roc_auc: 0.6350\n",
      "Epoch 1: val_roc_auc improved from -inf to 0.67116, saving model to best_model_iter_4.keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494/2494 [==============================] - 471s 189ms/step - loss: 0.9495 - accuracy: 0.2592 - roc_auc: 0.6350 - val_loss: 0.8306 - val_accuracy: 0.9959 - val_roc_auc: 0.6712 - lr: 1.8247e-06\n",
      "Epoch 2/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9510 - accuracy: 0.2580 - roc_auc: 0.6356\n",
      "Epoch 2: val_roc_auc improved from 0.67116 to 0.67209, saving model to best_model_iter_4.keras\n",
      "2494/2494 [==============================] - 473s 190ms/step - loss: 0.9510 - accuracy: 0.2580 - roc_auc: 0.6356 - val_loss: 0.8107 - val_accuracy: 0.9959 - val_roc_auc: 0.6721 - lr: 1.4030e-06\n",
      "Epoch 3/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9480 - accuracy: 0.2572 - roc_auc: 0.6359\n",
      "Epoch 3: val_roc_auc improved from 0.67209 to 0.67218, saving model to best_model_iter_4.keras\n",
      "2494/2494 [==============================] - 475s 190ms/step - loss: 0.9480 - accuracy: 0.2572 - roc_auc: 0.6359 - val_loss: 0.8193 - val_accuracy: 0.9960 - val_roc_auc: 0.6722 - lr: 1.0788e-06\n",
      "Epoch 4/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9560 - accuracy: 0.2606 - roc_auc: 0.6347\n",
      "Epoch 4: val_roc_auc did not improve from 0.67218\n",
      "2494/2494 [==============================] - 474s 190ms/step - loss: 0.9560 - accuracy: 0.2606 - roc_auc: 0.6347 - val_loss: 0.7979 - val_accuracy: 0.9956 - val_roc_auc: 0.6721 - lr: 8.2952e-07\n",
      "Epoch 5/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9530 - accuracy: 0.2603 - roc_auc: 0.6350\n",
      "Epoch 5: val_roc_auc did not improve from 0.67218\n",
      "2494/2494 [==============================] - 471s 189ms/step - loss: 0.9530 - accuracy: 0.2603 - roc_auc: 0.6350 - val_loss: 0.8261 - val_accuracy: 0.9960 - val_roc_auc: 0.6719 - lr: 6.3783e-07\n",
      "Epoch 6/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9533 - accuracy: 0.2595 - roc_auc: 0.6343\n",
      "Epoch 6: val_roc_auc did not improve from 0.67218\n",
      "2494/2494 [==============================] - 471s 189ms/step - loss: 0.9533 - accuracy: 0.2595 - roc_auc: 0.6343 - val_loss: 0.8356 - val_accuracy: 0.9961 - val_roc_auc: 0.6714 - lr: 4.9044e-07\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.8193 - accuracy: 0.9960 - roc_auc: 0.6722\n",
      "> run=4: Accuracy=99.597, ROC AUC=67.218\n",
      "Epoch 1/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9490 - accuracy: 0.2590 - roc_auc: 0.6350\n",
      "Epoch 1: val_roc_auc improved from -inf to 0.67168, saving model to best_model_iter_5.keras\n",
      "2494/2494 [==============================] - 473s 190ms/step - loss: 0.9490 - accuracy: 0.2590 - roc_auc: 0.6350 - val_loss: 0.8304 - val_accuracy: 0.9959 - val_roc_auc: 0.6717 - lr: 3.7711e-07\n",
      "Epoch 2/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9549 - accuracy: 0.2590 - roc_auc: 0.6338\n",
      "Epoch 2: val_roc_auc did not improve from 0.67168\n",
      "2494/2494 [==============================] - 472s 189ms/step - loss: 0.9549 - accuracy: 0.2590 - roc_auc: 0.6338 - val_loss: 0.8225 - val_accuracy: 0.9959 - val_roc_auc: 0.6713 - lr: 2.8997e-07\n",
      "Epoch 3/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9506 - accuracy: 0.2577 - roc_auc: 0.6354\n",
      "Epoch 3: val_roc_auc improved from 0.67168 to 0.67216, saving model to best_model_iter_5.keras\n",
      "2494/2494 [==============================] - 479s 192ms/step - loss: 0.9506 - accuracy: 0.2577 - roc_auc: 0.6354 - val_loss: 0.7963 - val_accuracy: 0.9957 - val_roc_auc: 0.6722 - lr: 2.2296e-07\n",
      "Epoch 4/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9522 - accuracy: 0.2589 - roc_auc: 0.6339\n",
      "Epoch 4: val_roc_auc did not improve from 0.67216\n",
      "2494/2494 [==============================] - 476s 191ms/step - loss: 0.9522 - accuracy: 0.2589 - roc_auc: 0.6339 - val_loss: 0.8190 - val_accuracy: 0.9958 - val_roc_auc: 0.6717 - lr: 1.7144e-07\n",
      "Epoch 5/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9477 - accuracy: 0.2602 - roc_auc: 0.6355\n",
      "Epoch 5: val_roc_auc did not improve from 0.67216\n",
      "2494/2494 [==============================] - 476s 191ms/step - loss: 0.9477 - accuracy: 0.2602 - roc_auc: 0.6355 - val_loss: 0.8085 - val_accuracy: 0.9957 - val_roc_auc: 0.6714 - lr: 1.3182e-07\n",
      "Epoch 6/10\n",
      "2494/2494 [==============================] - ETA: 0s - loss: 0.9486 - accuracy: 0.2600 - roc_auc: 0.6364\n",
      "Epoch 6: val_roc_auc did not improve from 0.67216\n",
      "2494/2494 [==============================] - 478s 192ms/step - loss: 0.9486 - accuracy: 0.2600 - roc_auc: 0.6364 - val_loss: 0.8164 - val_accuracy: 0.9959 - val_roc_auc: 0.6721 - lr: 1.0136e-07\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.7963 - accuracy: 0.9957 - roc_auc: 0.6722\n",
      "> run=5: Accuracy=99.567, ROC AUC=67.216\n",
      "Mean Accuracy=99.624, Mean ROC AUC=67.399\n"
     ]
    }
   ],
   "source": [
    "def load_pkl(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "    \n",
    "def to_lines(doc, vocab):\n",
    "    tokens = doc.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('', word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha() or word in ['not', 'no']]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def calculate_doc_length(lines):\n",
    "    return round(np.mean([len(s.split()) for s in lines]), None)\n",
    "\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded\n",
    "\n",
    "def define_model(vocab_size, length, kernels, weights, embedding_dim=100):\n",
    "    inputs = []\n",
    "    combine = []\n",
    "    for kernel in kernels:\n",
    "        in_layer = Input(shape=(length,))\n",
    "        embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[weights], trainable=True)(in_layer)\n",
    "        batch_norm1 = BatchNormalization()(embedding)\n",
    "        conv = Conv1D(filters=32, kernel_size=kernel, activation='relu', kernel_initializer='he_uniform')(batch_norm1)\n",
    "        drop = Dropout(0.5)(conv)\n",
    "        pool = MaxPool1D(pool_size=2)(drop)\n",
    "        flat = Flatten()(pool)\n",
    "        inputs.append(in_layer)\n",
    "        combine.append(flat)\n",
    "    merged = concatenate(combine)\n",
    "    dense = Dense(50, activation='relu', kernel_initializer='he_uniform')(merged)\n",
    "    batch_norm2 = BatchNormalization()(dense)\n",
    "    dense = Dropout(0.5)(batch_norm2)\n",
    "    outputs = Dense(6, activation='softmax', kernel_initializer='he_uniform')(dense)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=1000, decay_rate=0.9)\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy', AUC(name='roc_auc', multi_label=False)])\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='n_gram_multihead_cnn_model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length, batch_size=32):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.texts) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.texts[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        encoded_texts = encode_text(self.tokenizer, batch_texts, self.max_length)\n",
    "        return [encoded_texts] * kernels, np.array(batch_labels)\n",
    "\n",
    "def evaluate_model(model, Xtrain, ytrain, Xtest, ytest, kernels, tokenizer, length, n_repeats=5):\n",
    "    train_generator = DataGenerator(Xtrain, ytrain, tokenizer, length, batch_size=64)\n",
    "    test_generator = DataGenerator(Xtest, ytest, tokenizer, length, batch_size=64)\n",
    "    acc_scores, auc_scores = [], []\n",
    "    for i in range(1, n_repeats+1):\n",
    "        rlp = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        es = EarlyStopping(monitor='val_roc_auc', mode='max', patience=3, restore_best_weights=True)\n",
    "        model_name = 'best_model_iter_' + str(i) + '.keras'\n",
    "        mc = ModelCheckpoint(model_name, monitor='val_roc_auc', mode='max', verbose=1, save_best_only=True)\n",
    "        model.fit(train_generator, \n",
    "                  validation_data=test_generator, \n",
    "                  epochs=10, \n",
    "                  verbose=1, \n",
    "                  callbacks=[rlp, es, mc])\n",
    "        metrics = model.evaluate(test_generator, verbose=1)\n",
    "        acc, auc = metrics[1], metrics[2] \n",
    "        print(f'> run={i}: Accuracy={acc * 100:.3f}, ROC AUC={auc * 100:.3f}')\n",
    "        acc_scores.append(acc)\n",
    "        auc_scores.append(auc)\n",
    "    mean_acc, mean_auc = np.mean(acc_scores), np.mean(auc_scores)\n",
    "    print(f'Mean Accuracy={mean_acc * 100:.3f}, Mean ROC AUC={mean_auc * 100:.3f}')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # Load vocab\n",
    "    vocab_filename = 'vocab.text'\n",
    "    vocab = load_doc(vocab_filename)\n",
    "    vocab = set(vocab.split())\n",
    "\n",
    "    # Load datasets\n",
    "    ytrain, ytest = load_pkl('output.pkl')  \n",
    "    train_docs, test_docs = load_pkl('input.pkl')\n",
    "\n",
    "    # Clean training and test docs\n",
    "    trainLines = train_docs['comment_text'].apply(lambda x: to_lines(x, vocab)).astype(str)\n",
    "    testLines = test_docs['comment_text'].apply(lambda x: to_lines(x, vocab)).astype(str)\n",
    "\n",
    "    # Tokenize and encode datasets\n",
    "    tokenizer = create_tokenizer(trainLines)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    length = calculate_doc_length(trainLines)\n",
    "    print(f'Vocabulary size: {vocab_size}')\n",
    "    print(f'Average document length: {length}')\n",
    "\n",
    "    # load the embedding into memory\n",
    "    embeddings_index = {}\n",
    "    data_path = '/home/pmthisi/code_env/6_Deep_Learning_for_Natural_Language_Processing/glove_files'\n",
    "    f = open(f'{data_path}/glove.6B.100d.txt', mode='rt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vector' % len(embeddings_index))\n",
    "\n",
    "    # create a weight matrix for words in the training docs\n",
    "    embedding_matrix = np.random.uniform(-0.05, 0.05, size=(vocab_size, 100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # Define and evaluate model\n",
    "    kernels = [3, 5, 7]\n",
    "    model = define_model(vocab_size, length, kernels, embedding_matrix)\n",
    "    evaluate_model(model, trainLines.tolist(), ytrain, testLines.tolist(), ytest, kernels, tokenizer, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c031d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
