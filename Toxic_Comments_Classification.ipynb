{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a99a2db",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n",
    "\n",
    "- toxic\n",
    "- severe_toxic\n",
    "- obscene\n",
    "- threat\n",
    "- insult\n",
    "- identity_hate\n",
    "\n",
    "You must create a model which predicts a probability of each type of toxicity for each comment.\n",
    "\n",
    "File descriptions:\n",
    "- train.csv - the training set, contains comments with their binary labels\n",
    "- test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set - contains some comments which are not included in scoring.\n",
    "- sample_submission.csv - a sample submission file in the correct format\n",
    "- test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272db35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "from pickle import dump, load\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Conv1D, MaxPool1D, Flatten, Dropout, Dense, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def extract_zip_files(zip_file_path, extract_to_dir):\n",
    "    # Path to the zip file\n",
    "    zip_file_path = zip_file_path\n",
    "    # directory to extract contents\n",
    "    extract_to_dir = extract_to_dir\n",
    "    # Extracting the zip file\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            # Extract all contents to the specified directory\n",
    "            zip_ref.extractall(extract_to_dir)\n",
    "        print(f\"Contents of {zip_file_path} extracted successfully to {extract_to_dir}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {zip_file_path} does not exist.\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: The file {zip_file_path} is not a valid zip file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from words\n",
    "    tokens = [re_punc.sub('', word) for word in tokens]\n",
    "    # remove remaining words that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # standardize tokens\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# save a dataset to a file\n",
    "def save_dataset(dataset, filename):\n",
    "    dump(dataset, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "    \n",
    "# load a clean dataset\n",
    "def load_output(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7584e4b",
   "metadata": {},
   "source": [
    "# Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b1b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract main file\n",
    "zip_file_path = 'jigsaw-toxic-comment-classification-challenge.zip'\n",
    "extract_to_dir = './toxic_files'\n",
    "extract_zip_files(zip_file_path, extract_to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb31f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract training zip file:\n",
    "zip_file_path = './toxic_files/train.csv.zip'\n",
    "extract_to_dir = './toxic_files/train_file'\n",
    "extract_zip_files(zip_file_path, extract_to_dir)\n",
    "\n",
    "# extract test zip file:\n",
    "zip_file_path = './toxic_files/test.csv.zip'\n",
    "extract_to_dir = './toxic_files/test_file'\n",
    "extract_zip_files(zip_file_path, extract_to_dir)\n",
    "\n",
    "# extract \n",
    "zip_file_path = './toxic_files/test_labels.csv.zip'\n",
    "extract_to_dir = './toxic_files/test_file'\n",
    "extract_zip_files(zip_file_path, extract_to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339fbf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 159571 entries, 0000997932d777bf to fff46fc426af1f9a\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   comment_text   159571 non-null  object\n",
      " 1   toxic          159571 non-null  int64 \n",
      " 2   severe_toxic   159571 non-null  int64 \n",
      " 3   obscene        159571 non-null  int64 \n",
      " 4   threat         159571 non-null  int64 \n",
      " 5   insult         159571 non-null  int64 \n",
      " 6   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 9.7+ MB\n",
      "\n",
      "Data summary:\n",
      "Training dataset: 159571 rows, 7 columns\n",
      "Test dataset: 63978 rows, 7 columns\n",
      "Saved: output.pkl\n"
     ]
    }
   ],
   "source": [
    "# load extracted files\n",
    "train_df = pd.read_csv('./toxic_files/train_file/train.csv', index_col=0)\n",
    "test_doc = pd.read_csv('./toxic_files/test_file/test.csv')\n",
    "ytest = pd.read_csv('./toxic_files/test_file/test_labels.csv')\n",
    "\n",
    "train_df.info()\n",
    "\n",
    "# perform data remediation for the test set:\n",
    "test_df = pd.merge(test_doc, ytest, on='id', how='left')\n",
    "test_df.set_index('id', inplace=True)\n",
    "# remove none informative labels\n",
    "test_df = test_df[~(test_df == -1).any(axis=1)]\n",
    "# sumarise data sets:\n",
    "print('\\nData summary:')\n",
    "print(f'Training dataset: {train_df.shape[0]} rows, {train_df.shape[1]} columns')\n",
    "print(f'Test dataset: {test_df.shape[0]} rows, {test_df.shape[1]} columns')\n",
    "\n",
    "# split into input and output:\n",
    "train_docs, ytrain = train_df['comment_text'], train_df.iloc[:, -6:].values.astype(int)\n",
    "test_docs, ytest = test_df['comment_text'], test_df.iloc[:, -6:].values.astype(int)\n",
    "\n",
    "# save files:\n",
    "train_docs = train_docs.to_frame()\n",
    "test_docs = test_docs.to_frame()\n",
    "train_docs.to_csv('train_docs.csv'), test_docs.to_csv('test_docs.csv')\n",
    "save_dataset([ytrain, ytest], 'output.pkl')\n",
    "\n",
    "del train_df, test_doc, ytrain, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b750ed5",
   "metadata": {},
   "source": [
    "# Analysing most toxic tokens by toxicity type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa6ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class toxicity_analyser(object):\n",
    "    def __init__(self, df, most_common):\n",
    "        self.df = df\n",
    "        self.most_common = most_common\n",
    "        \n",
    "    def combine_words(self, word_list):\n",
    "        all_words = []\n",
    "        for word in word_list:\n",
    "            all_words += word\n",
    "        return all_words\n",
    "        \n",
    "    def create_word_cloud(self, col_name):\n",
    "        df = self.df[self.df[col_name] == 1]\n",
    "        tokens = df['comment_text'].apply(clean_doc)\n",
    "        reviewed_tokens = self.combine_words(tokens)\n",
    "        mostcommon = FreqDist(reviewed_tokens).most_common(self.most_common)\n",
    "        wordcloud = WordCloud(width=1500, height=800, background_color='white').generate(str(mostcommon))\n",
    "        fig = plt.figure(figsize=(30,10), facecolor='white')\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Top {self.most_common} Most Common Words', fontsize=25)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity = toxicity_analyser(train_df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d61d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94179a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('severe_toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('obscene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('threat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('insult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity.create_word_cloud('identity_hate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f574952",
   "metadata": {},
   "source": [
    "# Create Vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add doc to vocabulary\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines into a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "\n",
    "# add all docs to vocab\n",
    "add_doc_to_vocab('train_docs.csv', vocab)\n",
    "\n",
    "# print size of the vocab\n",
    "print(len(vocab))\n",
    "# print most common words\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "# keep token with a min occurance\n",
    "min_occurance = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "print(len(tokens))\n",
    "\n",
    "# save token to vocavulary\n",
    "save_list(tokens, 'vocab.text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed411c41",
   "metadata": {},
   "source": [
    "# Build model: n-gram multihead CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c3935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 89283\n",
      "Maximum document length: 1250\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1250)]               0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 1250)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 1250, 100)            8928300   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 1250, 100)            8928300   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 1241, 32)             32032     ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 1231, 32)             64032     ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 1241, 32)             0         ['conv1d[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 1231, 32)             0         ['conv1d_1[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1  (None, 620, 32)              0         ['dropout[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPoolin  (None, 615, 32)              0         ['dropout_1[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 19840)                0         ['max_pooling1d[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 19680)                0         ['max_pooling1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 39520)                0         ['flatten[0][0]',             \n",
      "                                                                     'flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 100)                  3952100   ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 6)                    606       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21905370 (83.56 MB)\n",
      "Trainable params: 21905370 (83.56 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from words\n",
    "    tokens = [re_punc.sub('', word) for word in tokens]\n",
    "    # remove remaining words that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # standardize tokens\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    #filter out tokens not in the vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(filename, vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc, vocab)\n",
    "    return tokens\n",
    "\n",
    "   \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded\n",
    "\n",
    "# Define model\n",
    "def define_model(vocab_size, length, kernels, embedding_dim=100):\n",
    "    inputs = []\n",
    "    combine = []\n",
    "    for kernel in kernels:\n",
    "        in_layer = Input(shape=(length,))\n",
    "        embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(in_layer)\n",
    "        conv = Conv1D(filters=32, kernel_size=kernel, activation='relu')(embedding)\n",
    "        conv = Conv1D(filters=32, kernel_size=kernel, activation='relu')(conv)\n",
    "        drop = Dropout(0.5)(conv2)\n",
    "        pool = MaxPool1D(pool_size=2)(drop2)\n",
    "        flat = Flatten()(pool)\n",
    "        inputs.append(in_layer)\n",
    "        combine.append(flat)\n",
    "    merged = concatenate(combine)\n",
    "    dense1 = Dense(100, activation='relu')(merged)\n",
    "    outputs = Dense(6, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=1000, decay_rate=0.9)\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', 'roc_auc'])\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='n_gram_multihead_cnn_model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, Xtrain, ytrain, Xtest, ytest, kernels, n_repeats=5):\n",
    "    channels = len(kernels)\n",
    "    train_pattern = [Xtrain] * channels\n",
    "    test_pattern = [Xtest] * channels\n",
    "    scores = []\n",
    "    for i in range(1, n_repeats + 1):\n",
    "        rlp = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5)\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', patience=5, restore_best_weights=True)\n",
    "        model.fit(train_pattern, ytrain, validation_split=0.2, epochs=20, batch_size=32, verbose=1, callbacks=[rlp, es])\n",
    "        _, acc = model.evaluate(test_pattern, ytest, verbose=0)\n",
    "        if i == 1 or i % 5 == 0:\n",
    "            print(f'Run {i}: Test Accuracy: {acc * 100:.3f}')\n",
    "        scores.append(acc)\n",
    "    mean_score = np.mean(scores)\n",
    "    print(f'Mean Accuracy: {mean_score * 100:.3f}')\n",
    "    \n",
    "# load vocab\n",
    "vocab_filename = 'vocab.text'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "    \n",
    "# Load datasets\n",
    "ytrain, ytest = load_output('output.pkl')\n",
    "\n",
    "# clean training and test docs\n",
    "trainLines = train_docs['comment_text'].apply(lambda x: clean_doc(x, vocab)).astype(str)\n",
    "testLines = test_docs['comment_text'].apply(lambda x: clean_doc(x, vocab)).astype(str)\n",
    "\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "length = max_length(trainLines)\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "print(f'Maximum document length: {length}')\n",
    "\n",
    "# Encode datasets\n",
    "Xtrain = encode_text(tokenizer, trainLines.tolist(), length)  # Convert Series to list\n",
    "Xtest = encode_text(tokenizer, testLines.tolist(), length)  # Convert Series to list\n",
    "\n",
    "# Define and evaluate model\n",
    "kernels = [10, 20]\n",
    "model = define_model(vocab_size, length, kernels)\n",
    "evaluate_model(model, Xtrain, ytrain, Xtest, ytest, kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4329f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0bc50a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928b320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
